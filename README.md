<p align="center">
  <img src='https://github.com/tinydance/tinydance.github.io/blob/main/img/DanceMuse.svg'>
</p>

[DanceMuse](https://github.com/tinydance/DanceMuse) is a bash tool for preprocessing data for music-to-dance generation built on top of [Slurm](https://slurm.schedmd.com/sbatch.html), [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose), and [FFmpeg](https://www.ffmpeg.org/). The current version is costumized for 
[DanceRevolution](https://github.com/stonyhu/DanceRevolution), aimed at handling all model preprocessing and testing using this specific music-to-dance model. DanceMuse is a tool to augment artistic thought using machine learning. By using user-defined data and audio, artists are able to explore the boundaries between technology and art. 

DanceMuse is developed by [Yuval Ofek](https://github.com/yuvalofek), [Jason Kurian](https://github.com/jkurian49), and [Yuecen (Crystal) Wang](https://github.com/CrystalWang1225)
to facilitate the interaction between artists, specifically choreographers and dancers, and the deep learning community. 

See project website [here](https://tinydance.github.io/).

## Using DanceMuse
<p align="center">
  <img src='/imgs/use_dancemuse.svg' width=600>
</p>
DanceMuse does not create art, but aims at giving suggestions and helping artists explore dance and audio in a new way. We propose a 4 step process of interacting with the system to allow artists iteratively refine their ideas and thoughts, shown in the figure above. 



## Dance Outputs
<p align="center">
  <img src='https://github.com/tinydance/tinydance.github.io/blob/main/img/ballet-sample.gif' width=600>
</p>
